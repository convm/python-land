{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping & Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "response = urllib.request.urlopen('http://php.net/')\n",
    "html = response.read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "text = soup.get_text(strip=True)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens= tokenizer.tokenize(text)\n",
    "#tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(tokens)\n",
    "count = 0\n",
    "for key, val in freq.items():\n",
    "    print(\"[{}] {}: {}\".format(count+1, key, val))\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "        \n",
    "freq.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "\n",
    "clean_tokens = tokens[:]\n",
    "for token in tokens:\n",
    "    if token in sw:\n",
    "        clean_tokens.remove(token)\n",
    "        \n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "freq.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('increase'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "print('Training entries: {}, labels: {}'.format(len(train_data), len(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "word_index = { k:(v+3) for k, v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2 # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([value, key] for key, value in word_index.items())\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], padding='post', maxlen=256)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], padding='post', maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train,\n",
    "                    epochs=10, batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BQ Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panda - Read Excel into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies for this session\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Excel\n",
    "#df_original = pd.read_excel('BQ Analysis for Python.xlsx', sheet_name='Project BQ')\n",
    "#df_original.columns = ['Contract', 'BqRef', 'BqTrade', 'BqSubTrade', 'Heading', 'SubHeading', \n",
    "#              'ItemHeading', 'ItemDesc', 'Qty', 'UOM', 'AcCostID', 'AcTrade', 'AcSubTrade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_original.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('data.csv')\n",
    "df_original.columns = ['Contract', 'BqRef', 'BqTrade', 'BqSubTrade', 'Heading', 'SubHeading', \n",
    "              'ItemHeading', 'ItemDesc', 'Qty', 'UOM', 'AcCostID', 'AcTrade', 'AcSubTrade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Missing Data\n",
    "df = df_original.copy()\n",
    "# Skip some troublesome Trades\n",
    "#df = df[(df.AcTrade != 'IP') & (df.AcTrade != 'SU') & (df.AcTrade != 'DA') & (df.AcTrade != 'DA') & (df.AcTrade != 'AS')]\n",
    "df = df.fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine columns into \"Super Description\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['SuperDesc'] = df.BqTrade + \" \" + df.Heading + \" \" + df.SubHeading + \" \" + df.ItemHeading + \" \" + df.ItemDesc\n",
    "df['SuperDesc'] = df.BqTrade + ' ' + df.Heading + ' ' + df.ItemHeading + ' ' + df.ItemDesc\n",
    "print(df.SuperDesc.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Categorical to Numerical transformation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df.AcTrade.astype('category')\n",
    "df['AcTradeCode'] = pd.Categorical(df.AcTrade).codes\n",
    "ac_trade_categories = pd.Categorical(df.AcTrade)\n",
    "#print(ac_trade_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ac_trade_categories.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test with full population\n",
    "x = df.SuperDesc\n",
    "y = df.AcTradeCode\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "x, y = shuffle(x, y, random_state=99)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set Manipulation\n",
    "x_test = x\n",
    "y_test = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df.Contract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test with Target Prject\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "target_contract = 'FL49'\n",
    "x_train = df[df.Contract != target_contract].SuperDesc.values\n",
    "y_train = df[df.Contract != target_contract].AcTradeCode.values\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=99)\n",
    "\n",
    "x_test = df[df.Contract == target_contract].SuperDesc.values\n",
    "y_test = df[df.Contract == target_contract].AcTradeCode.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training - Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Model Training - Support Vector Machine\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,2), stop_words='english', analyzer='word', binary=True)),\n",
    "    #('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-5, random_state=99, max_iter=200, tol=1e-6))\n",
    "    #('clf', RandomForestClassifier(n_estimators=100, max_depth=20, random_state=99))\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "text_clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = text_clf.predict(x_test)\n",
    "print(np.mean(y_pred == y_test)) # Score\n",
    "print((y_pred != y_test).sum()) # Count of incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_report_set = set(y_test).union(set(y_pred))\n",
    "y_report_names = ac_trade_categories.categories[list(y_report_set)]\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=y_report_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(text_clf, x_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict(enumerate(pd.Categorical(df.AcTrade).categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Pred'] = [ac_trade_categories[i] for i in y_pred]\n",
    "df_result = df[df.AcTrade != df.Pred]\n",
    "print(df_result)\n",
    "#df_result.to_excel('result.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Fitting\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "text_clf.fit(x_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = text_clf.predict(x_test)\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_pred, y, target_names=ac_trade_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training - Multinomial Naive Bayes (Verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "x_train_count = count_vect.fit_transform(x_train)\n",
    "x_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'concrete grade sign way'\n",
    "temp = [count_vect.vocabulary_.get(i) for i in text.split()]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(x_train_count)\n",
    "x_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "clf = MultinomialNB().fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "x_test_count = count_vect.transform(x_test)\n",
    "x_test_tfidf = tfidf_transformer.transform(x_test_count)\n",
    "\n",
    "y_pred = clf.predict(x_test_tfidf)\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
